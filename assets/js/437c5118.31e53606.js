"use strict";(self.webpackChunk_elizaos_docs=self.webpackChunk_elizaos_docs||[]).push([[86519],{44202:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>t,contentTitle:()=>o,default:()=>h,frontMatter:()=>a,metadata:()=>l,toc:()=>c});const l=JSON.parse('{"id":"plugins/llama","title":"@elizaos/plugin-llama","description":"Core LLaMA plugin for Eliza OS that provides local Large Language Model capabilities.","source":"@site/packages/plugins/llama.md","sourceDirName":"plugins","slug":"/plugins/llama","permalink":"/packages/plugins/llama","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"defaultSidebar","previous":{"title":"@elizaos/plugin-lit","permalink":"/packages/plugins/lit"},"next":{"title":"Massa Plugin","permalink":"/packages/plugins/massa"}}');var s=i(31085),r=i(71184);const a={},o="@elizaos/plugin-llama",t={},c=[{value:"Overview",id:"overview",level:2},{value:"Features",id:"features",level:2},{value:"Installation",id:"installation",level:2},{value:"Configuration",id:"configuration",level:2},{value:"Core Settings",id:"core-settings",level:3},{value:"Usage",id:"usage",level:2},{value:"Services",id:"services",level:2},{value:"LlamaService",id:"llamaservice",level:3},{value:"Technical Details",id:"technical-details",level:4},{value:"Features",id:"features-1",level:4},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Common Issues",id:"common-issues",level:3},{value:"Debug Mode",id:"debug-mode",level:3},{value:"System Requirements",id:"system-requirements",level:3},{value:"Performance Optimization",id:"performance-optimization",level:3},{value:"Support",id:"support",level:2},{value:"Credits",id:"credits",level:2},{value:"License",id:"license",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"elizaosplugin-llama",children:"@elizaos/plugin-llama"})}),"\n",(0,s.jsx)(n.p,{children:"Core LLaMA plugin for Eliza OS that provides local Large Language Model capabilities."}),"\n",(0,s.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(n.p,{children:"The LLaMA plugin serves as a foundational component of Eliza OS, providing local LLM capabilities using LLaMA models. It enables efficient and customizable text generation with both CPU and GPU support."}),"\n",(0,s.jsx)(n.h2,{id:"features",children:"Features"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Local LLM Support"}),": Run LLaMA models locally"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"GPU Acceleration"}),": CUDA support for faster inference"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Flexible Configuration"}),": Customizable parameters for text generation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Message Queuing"}),": Efficient handling of multiple requests"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Automatic Model Management"}),": Download and verification systems"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"installation",children:"Installation"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"npm install @elizaos/plugin-llama\n"})}),"\n",(0,s.jsx)(n.h2,{id:"configuration",children:"Configuration"}),"\n",(0,s.jsx)(n.p,{children:"The plugin can be configured through environment variables:"}),"\n",(0,s.jsx)(n.h3,{id:"core-settings",children:"Core Settings"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-env",children:"LLAMALOCAL_PATH=your_model_storage_path\nOLLAMA_MODEL=optional_ollama_model_name\n"})}),"\n",(0,s.jsx)(n.h2,{id:"usage",children:"Usage"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"import { createLlamaPlugin } from '@elizaos/plugin-llama';\n\n// Initialize the plugin\nconst llamaPlugin = createLlamaPlugin();\n\n// Register with Eliza OS\nelizaos.registerPlugin(llamaPlugin);\n"})}),"\n",(0,s.jsx)(n.h2,{id:"services",children:"Services"}),"\n",(0,s.jsx)(n.h3,{id:"llamaservice",children:"LlamaService"}),"\n",(0,s.jsx)(n.p,{children:"Provides local LLM capabilities using LLaMA models."}),"\n",(0,s.jsx)(n.h4,{id:"technical-details",children:"Technical Details"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Model"}),": Hermes-3-Llama-3.1-8B (8-bit quantized)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Source"}),": Hugging Face (NousResearch/Hermes-3-Llama-3.1-8B-GGUF)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Context Size"}),": 8192 tokens"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Inference"}),": CPU and GPU (CUDA) support"]}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"features-1",children:"Features"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Text Generation"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Completion-style inference"}),"\n",(0,s.jsx)(n.li,{children:"Temperature control"}),"\n",(0,s.jsx)(n.li,{children:"Stop token configuration"}),"\n",(0,s.jsx)(n.li,{children:"Frequency and presence penalties"}),"\n",(0,s.jsx)(n.li,{children:"Maximum token limit control"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Model Management"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Automatic model downloading"}),"\n",(0,s.jsx)(n.li,{children:"Model file verification"}),"\n",(0,s.jsx)(n.li,{children:"Automatic retry on initialization failures"}),"\n",(0,s.jsx)(n.li,{children:"GPU detection for acceleration"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Performance"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Message queuing system"}),"\n",(0,s.jsx)(n.li,{children:"CUDA acceleration when available"}),"\n",(0,s.jsx)(n.li,{children:"Configurable context size"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,s.jsx)(n.h3,{id:"common-issues",children:"Common Issues"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.strong,{children:"Model Initialization Failures"})}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"Error: Model initialization failed\n"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Verify model file exists and is not corrupted"}),"\n",(0,s.jsx)(n.li,{children:"Check available system memory"}),"\n",(0,s.jsx)(n.li,{children:"Ensure CUDA is properly configured (if using GPU)"}),"\n"]}),"\n",(0,s.jsxs)(n.ol,{start:"2",children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.strong,{children:"Performance Issues"})}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"Warning: No CUDA detected - local response will be slow\n"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Verify CUDA installation if using GPU"}),"\n",(0,s.jsx)(n.li,{children:"Check system resources"}),"\n",(0,s.jsx)(n.li,{children:"Consider reducing context size"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"debug-mode",children:"Debug Mode"}),"\n",(0,s.jsx)(n.p,{children:"Enable debug logging for detailed troubleshooting:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"process.env.DEBUG = 'eliza:plugin-llama:*';\n"})}),"\n",(0,s.jsx)(n.h3,{id:"system-requirements",children:"System Requirements"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Node.js 16.x or higher"}),"\n",(0,s.jsx)(n.li,{children:"Minimum 8GB RAM recommended"}),"\n",(0,s.jsx)(n.li,{children:"CUDA-compatible GPU (optional, for acceleration)"}),"\n",(0,s.jsx)(n.li,{children:"Sufficient storage for model files"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Model Selection"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Choose appropriate model size"}),"\n",(0,s.jsx)(n.li,{children:"Use quantized versions when possible"}),"\n",(0,s.jsx)(n.li,{children:"Balance quality vs speed"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Resource Management"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Monitor memory usage"}),"\n",(0,s.jsx)(n.li,{children:"Configure appropriate context size"}),"\n",(0,s.jsx)(n.li,{children:"Optimize batch processing"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"GPU Utilization"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Enable CUDA when available"}),"\n",(0,s.jsx)(n.li,{children:"Monitor GPU memory"}),"\n",(0,s.jsx)(n.li,{children:"Balance CPU/GPU workload"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"support",children:"Support"}),"\n",(0,s.jsx)(n.p,{children:"For issues and feature requests, please:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Check the troubleshooting guide above"}),"\n",(0,s.jsx)(n.li,{children:"Review existing GitHub issues"}),"\n",(0,s.jsxs)(n.li,{children:["Submit a new issue with:","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"System information"}),"\n",(0,s.jsx)(n.li,{children:"Error logs"}),"\n",(0,s.jsx)(n.li,{children:"Steps to reproduce"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"credits",children:"Credits"}),"\n",(0,s.jsx)(n.p,{children:"This plugin integrates with and builds upon:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://github.com/facebookresearch/llama",children:"LLaMA"})," - Base language model"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://github.com/withcatai/node-llama-cpp",children:"node-llama-cpp"})," - Node.js bindings"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://github.com/ggerganov/ggml",children:"GGUF"})," - Model format"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Special thanks to:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"The LLaMA community for model development"}),"\n",(0,s.jsx)(n.li,{children:"The Node.js community for tooling support"}),"\n",(0,s.jsx)(n.li,{children:"The Eliza community for testing and feedback"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"license",children:"License"}),"\n",(0,s.jsx)(n.p,{children:"This plugin is part of the Eliza project. See the main project repository for license information."})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},71184:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>o});var l=i(14041);const s={},r=l.createContext(s);function a(e){const n=l.useContext(r);return l.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),l.createElement(r.Provider,{value:n},e.children)}}}]);