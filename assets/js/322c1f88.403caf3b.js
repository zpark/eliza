"use strict";(self.webpackChunk_elizaos_docs=self.webpackChunk_elizaos_docs||[]).push([[97084],{4397:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>d,frontMatter:()=>a,metadata:()=>s,toc:()=>l});const s=JSON.parse('{"id":"technical/advanced/performance","title":"Performance Guide","description":"Optimization techniques and performance best practices for ElizaOS","source":"@site/docs/technical/advanced/performance.md","sourceDirName":"technical/advanced","slug":"/technical/advanced/performance","permalink":"/docs/technical/advanced/performance","draft":false,"unlisted":false,"editUrl":"https://github.com/elizaos/eliza/tree/develop/packages/docs/docs/technical/advanced/performance.md","tags":[],"version":"current","lastUpdatedBy":"SYMBaiEX","lastUpdatedAt":1751753321000,"frontMatter":{"title":"Performance Guide","description":"Optimization techniques and performance best practices for ElizaOS"},"sidebar":"technicalSidebar","previous":{"title":"\u2728 Best Practices","permalink":"/docs/technical/advanced/best-practices"},"next":{"title":"\u2753 Technical FAQ","permalink":"/docs/technical/faq"}}');var i=t(31085),r=t(71184);const a={title:"Performance Guide",description:"Optimization techniques and performance best practices for ElizaOS"},o="Performance Guide",c={},l=[{value:"Overview",id:"overview",level:2},{value:"Performance Metrics",id:"performance-metrics",level:2},{value:"Key Metrics to Monitor",id:"key-metrics-to-monitor",level:3},{value:"Performance Monitoring",id:"performance-monitoring",level:3},{value:"Response Time Optimization",id:"response-time-optimization",level:2},{value:"1. Caching Strategies",id:"1-caching-strategies",level:3},{value:"Memory Caching",id:"memory-caching",level:4},{value:"Multi-Level Caching",id:"multi-level-caching",level:4},{value:"2. Query Optimization",id:"2-query-optimization",level:3},{value:"Batch Processing",id:"batch-processing",level:4},{value:"Database Query Optimization",id:"database-query-optimization",level:4},{value:"3. Async Operation Optimization",id:"3-async-operation-optimization",level:3},{value:"Parallel Processing",id:"parallel-processing",level:4},{value:"Stream Processing",id:"stream-processing",level:4},{value:"Memory Optimization",id:"memory-optimization",level:2},{value:"1. Memory Management",id:"1-memory-management",level:3},{value:"2. Object Pooling",id:"2-object-pooling",level:3},{value:"3. Memory Leak Prevention",id:"3-memory-leak-prevention",level:3},{value:"Scalability Patterns",id:"scalability-patterns",level:2},{value:"1. Horizontal Scaling",id:"1-horizontal-scaling",level:3},{value:"2. Load Balancing",id:"2-load-balancing",level:3},{value:"3. Message Queue Integration",id:"3-message-queue-integration",level:3},{value:"Database Performance",id:"database-performance",level:2},{value:"1. Connection Pooling",id:"1-connection-pooling",level:3},{value:"2. Query Optimization",id:"2-query-optimization-1",level:3},{value:"Network Optimization",id:"network-optimization",level:2},{value:"1. HTTP/2 and Compression",id:"1-http2-and-compression",level:3},{value:"2. WebSocket Optimization",id:"2-websocket-optimization",level:3},{value:"Cost Optimization",id:"cost-optimization",level:2},{value:"1. API Call Optimization",id:"1-api-call-optimization",level:3},{value:"2. Resource Usage Optimization",id:"2-resource-usage-optimization",level:3},{value:"Monitoring and Profiling",id:"monitoring-and-profiling",level:2},{value:"1. Performance Profiling",id:"1-performance-profiling",level:3},{value:"2. Real-time Monitoring",id:"2-real-time-monitoring",level:3},{value:"Best Practices Summary",id:"best-practices-summary",level:2},{value:"Related Documentation",id:"related-documentation",level:2}];function m(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"performance-guide",children:"Performance Guide"})}),"\n",(0,i.jsx)(n.p,{children:"This guide covers performance optimization techniques for ElizaOS applications, from basic optimizations to advanced scaling strategies."}),"\n",(0,i.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,i.jsx)(n.p,{children:"Performance optimization in ElizaOS focuses on:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Response time optimization"}),"\n",(0,i.jsx)(n.li,{children:"Memory efficiency"}),"\n",(0,i.jsx)(n.li,{children:"Scalability patterns"}),"\n",(0,i.jsx)(n.li,{children:"Resource utilization"}),"\n",(0,i.jsx)(n.li,{children:"Cost optimization"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"performance-metrics",children:"Performance Metrics"}),"\n",(0,i.jsx)(n.h3,{id:"key-metrics-to-monitor",children:"Key Metrics to Monitor"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-typescript",children:"interface PerformanceMetrics {\n  // Response metrics\n  responseTime: number; // Average response time (ms)\n  p95ResponseTime: number; // 95th percentile response time\n  p99ResponseTime: number; // 99th percentile response time\n\n  // Throughput metrics\n  messagesPerSecond: number; // Message processing rate\n  actionsPerSecond: number; // Action execution rate\n\n  // Resource metrics\n  memoryUsage: number; // Memory usage (MB)\n  cpuUsage: number; // CPU usage (%)\n  activeConnections: number; // Active connections\n\n  // Cache metrics\n  cacheHitRate: number; // Cache hit percentage\n  cacheMissRate: number; // Cache miss percentage\n\n  // Error metrics\n  errorRate: number; // Error percentage\n  timeoutRate: number; // Timeout percentage\n}\n"})}),"\n",(0,i.jsx)(n.h3,{id:"performance-monitoring",children:"Performance Monitoring"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-typescript",children:"class PerformanceMonitor {\n  private metrics: PerformanceMetrics;\n  private intervals: Map<string, NodeJS.Timer> = new Map();\n\n  start(): void {\n    // Response time tracking\n    this.trackResponseTimes();\n\n    // Resource monitoring\n    this.monitorResources();\n\n    // Cache performance\n    this.trackCachePerformance();\n\n    // Report metrics\n    this.intervals.set(\n      'report',\n      setInterval(() => {\n        this.reportMetrics();\n      }, 60000)\n    ); // Every minute\n  }\n\n  private trackResponseTimes(): void {\n    const histogram = new Histogram();\n\n    runtime.on('message:processed', ({ duration }) => {\n      histogram.record(duration);\n\n      this.metrics.responseTime = histogram.mean();\n      this.metrics.p95ResponseTime = histogram.percentile(95);\n      this.metrics.p99ResponseTime = histogram.percentile(99);\n    });\n  }\n\n  private monitorResources(): void {\n    this.intervals.set(\n      'resources',\n      setInterval(() => {\n        const usage = process.memoryUsage();\n        this.metrics.memoryUsage = usage.heapUsed / 1024 / 1024;\n\n        // CPU usage requires additional monitoring\n        this.metrics.cpuUsage = this.getCPUUsage();\n      }, 5000)\n    );\n  }\n}\n"})}),"\n",(0,i.jsx)(n.h2,{id:"response-time-optimization",children:"Response Time Optimization"}),"\n",(0,i.jsx)(n.h3,{id:"1-caching-strategies",children:"1. Caching Strategies"}),"\n",(0,i.jsx)(n.h4,{id:"memory-caching",children:"Memory Caching"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-typescript",children:"class MemoryCache<T> {\n  private cache = new Map<string, CacheEntry<T>>();\n  private readonly maxSize: number;\n  private readonly ttl: number;\n\n  constructor(options: CacheOptions) {\n    this.maxSize = options.maxSize || 1000;\n    this.ttl = options.ttl || 5 * 60 * 1000; // 5 minutes\n  }\n\n  async get(key: string, fetcher?: () => Promise<T>): Promise<T | null> {\n    const entry = this.cache.get(key);\n\n    // Cache hit\n    if (entry && !this.isExpired(entry)) {\n      entry.hits++;\n      return entry.value;\n    }\n\n    // Cache miss with fetcher\n    if (fetcher) {\n      const value = await fetcher();\n      this.set(key, value);\n      return value;\n    }\n\n    return null;\n  }\n\n  set(key: string, value: T): void {\n    // Evict if at capacity\n    if (this.cache.size >= this.maxSize) {\n      this.evictLRU();\n    }\n\n    this.cache.set(key, {\n      value,\n      timestamp: Date.now(),\n      hits: 0,\n    });\n  }\n\n  private evictLRU(): void {\n    let lruKey: string | null = null;\n    let lruTime = Infinity;\n\n    for (const [key, entry] of this.cache) {\n      const lastAccess = entry.timestamp + entry.hits * 1000;\n      if (lastAccess < lruTime) {\n        lruTime = lastAccess;\n        lruKey = key;\n      }\n    }\n\n    if (lruKey) {\n      this.cache.delete(lruKey);\n    }\n  }\n}\n"})}),"\n",(0,i.jsx)(n.h4,{id:"multi-level-caching",children:"Multi-Level Caching"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-typescript",children:"class MultiLevelCache {\n  private l1Cache: MemoryCache<any>; // Fast, small\n  private l2Cache: RedisCache; // Slower, larger\n\n  async get(key: string): Promise<any> {\n    // Try L1 first\n    let value = await this.l1Cache.get(key);\n    if (value) return value;\n\n    // Try L2\n    value = await this.l2Cache.get(key);\n    if (value) {\n      // Promote to L1\n      await this.l1Cache.set(key, value);\n      return value;\n    }\n\n    return null;\n  }\n\n  async set(key: string, value: any, options?: CacheOptions): Promise<void> {\n    // Write to both levels\n    await Promise.all([this.l1Cache.set(key, value), this.l2Cache.set(key, value, options)]);\n  }\n}\n"})}),"\n",(0,i.jsx)(n.h3,{id:"2-query-optimization",children:"2. Query Optimization"}),"\n",(0,i.jsx)(n.h4,{id:"batch-processing",children:"Batch Processing"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-typescript",children:"class BatchProcessor<T, R> {\n  private batch: Array<{ item: T; resolve: (value: R) => void }> = [];\n  private timer: NodeJS.Timeout | null = null;\n\n  constructor(\n    private processor: (items: T[]) => Promise<R[]>,\n    private options: BatchOptions = {}\n  ) {\n    this.options.maxSize = options.maxSize || 100;\n    this.options.maxWait = options.maxWait || 50;\n  }\n\n  async process(item: T): Promise<R> {\n    return new Promise((resolve) => {\n      this.batch.push({ item, resolve });\n\n      if (this.batch.length >= this.options.maxSize!) {\n        this.flush();\n      } else if (!this.timer) {\n        this.timer = setTimeout(() => this.flush(), this.options.maxWait!);\n      }\n    });\n  }\n\n  private async flush(): Promise<void> {\n    if (this.timer) {\n      clearTimeout(this.timer);\n      this.timer = null;\n    }\n\n    if (this.batch.length === 0) return;\n\n    const currentBatch = this.batch;\n    this.batch = [];\n\n    try {\n      const items = currentBatch.map((b) => b.item);\n      const results = await this.processor(items);\n\n      currentBatch.forEach((b, i) => {\n        b.resolve(results[i]);\n      });\n    } catch (error) {\n      currentBatch.forEach((b) => {\n        b.resolve(Promise.reject(error));\n      });\n    }\n  }\n}\n\n// Usage\nconst embeddingProcessor = new BatchProcessor(\n  async (texts: string[]) => {\n    return await generateEmbeddings(texts); // Batch API call\n  },\n  { maxSize: 50, maxWait: 100 }\n);\n"})}),"\n",(0,i.jsx)(n.h4,{id:"database-query-optimization",children:"Database Query Optimization"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-typescript",children:"class OptimizedDatabase {\n  // Use prepared statements\n  private statements = new Map<string, PreparedStatement>();\n\n  async query(sql: string, params?: any[]): Promise<any> {\n    // Use prepared statement if available\n    let statement = this.statements.get(sql);\n    if (!statement) {\n      statement = await this.prepare(sql);\n      this.statements.set(sql, statement);\n    }\n\n    return await statement.execute(params);\n  }\n\n  // Optimize N+1 queries\n  async getMessagesWithUsers(messageIds: string[]): Promise<any[]> {\n    // Bad: N+1 queries\n    // for (const id of messageIds) {\n    //   const message = await this.query('SELECT * FROM messages WHERE id = ?', [id]);\n    //   const user = await this.query('SELECT * FROM users WHERE id = ?', [message.entityId]);\n    // }\n\n    // Good: Single query with JOIN\n    return await this.query(\n      `\n      SELECT m.*, u.name as userName, u.avatar as userAvatar\n      FROM messages m\n      JOIN users u ON m.userId = u.id\n      WHERE m.id = ANY(?)\n    `,\n      [messageIds]\n    );\n  }\n\n  // Use indexes effectively\n  async createIndexes(): Promise<void> {\n    await this.query(`\n      CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_messages_user_time \n      ON messages(userId, timestamp DESC)\n    `);\n\n    await this.query(`\n      CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_memories_embedding \n      ON memories USING ivfflat (embedding vector_cosine_ops)\n    `);\n  }\n}\n"})}),"\n",(0,i.jsx)(n.h3,{id:"3-async-operation-optimization",children:"3. Async Operation Optimization"}),"\n",(0,i.jsx)(n.h4,{id:"parallel-processing",children:"Parallel Processing"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-typescript",children:"class ParallelProcessor {\n  async processMessages(messages: Message[]): Promise<ProcessedMessage[]> {\n    // Bad: Sequential processing\n    // const results = [];\n    // for (const message of messages) {\n    //   results.push(await this.processMessage(message));\n    // }\n\n    // Good: Parallel processing with concurrency limit\n    return await pMap(messages, (message) => this.processMessage(message), { concurrency: 10 });\n  }\n\n  async processWithDependencies(tasks: Task[]): Promise<any[]> {\n    // Build dependency graph\n    const graph = this.buildDependencyGraph(tasks);\n\n    // Process in parallel respecting dependencies\n    const results = new Map<string, any>();\n    const processing = new Set<string>();\n\n    const processTask = async (task: Task): Promise<any> => {\n      // Wait for dependencies\n      await Promise.all(\n        task.dependencies.map((dep) =>\n          results.has(dep) ? Promise.resolve() : processTask(findTask(dep))\n        )\n      );\n\n      // Process task\n      if (!processing.has(task.id)) {\n        processing.add(task.id);\n        const result = await task.execute();\n        results.set(task.id, result);\n        return result;\n      }\n\n      // Wait for existing processing\n      while (!results.has(task.id)) {\n        await sleep(10);\n      }\n      return results.get(task.id);\n    };\n\n    // Start all tasks\n    await Promise.all(tasks.map(processTask));\n\n    return Array.from(results.values());\n  }\n}\n"})}),"\n",(0,i.jsx)(n.h4,{id:"stream-processing",children:"Stream Processing"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-typescript",children:"class StreamProcessor {\n  async processLargeDataset(\n    source: AsyncIterable<any>,\n    transform: (item: any) => Promise<any>\n  ): Promise<void> {\n    const pipeline = new TransformStream({\n      async transform(chunk, controller) {\n        const result = await transform(chunk);\n        controller.enqueue(result);\n      },\n    });\n\n    // Process in streaming fashion\n    const reader = source[Symbol.asyncIterator]();\n    const writer = pipeline.writable.getWriter();\n\n    try {\n      while (true) {\n        const { done, value } = await reader.next();\n        if (done) break;\n\n        await writer.write(value);\n      }\n      await writer.close();\n    } catch (error) {\n      await writer.abort(error);\n      throw error;\n    }\n  }\n}\n"})}),"\n",(0,i.jsx)(n.h2,{id:"memory-optimization",children:"Memory Optimization"}),"\n",(0,i.jsx)(n.h3,{id:"1-memory-management",children:"1. Memory Management"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-typescript",children:"class MemoryManager {\n  private readonly maxHeapUsage = 0.8; // 80% of available heap\n\n  startMonitoring(): void {\n    setInterval(() => {\n      const usage = process.memoryUsage();\n      const heapUsedPercent = usage.heapUsed / usage.heapTotal;\n\n      if (heapUsedPercent > this.maxHeapUsage) {\n        this.performGarbageCollection();\n      }\n    }, 30000); // Every 30 seconds\n  }\n\n  private performGarbageCollection(): void {\n    // Force garbage collection if available\n    if (global.gc) {\n      global.gc();\n    }\n\n    // Clear caches\n    this.clearOldCaches();\n\n    // Reduce memory footprint\n    this.compactDataStructures();\n  }\n\n  private clearOldCaches(): void {\n    // Clear expired entries from all caches\n    for (const cache of this.caches) {\n      cache.cleanup();\n    }\n  }\n\n  private compactDataStructures(): void {\n    // Compact large arrays and objects\n    for (const store of this.dataStores) {\n      store.compact();\n    }\n  }\n}\n"})}),"\n",(0,i.jsx)(n.h3,{id:"2-object-pooling",children:"2. Object Pooling"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-typescript",children:"class ObjectPool<T> {\n  private available: T[] = [];\n  private inUse = new Set<T>();\n\n  constructor(\n    private factory: () => T,\n    private reset: (obj: T) => void,\n    private maxSize: number = 100\n  ) {\n    // Pre-populate pool\n    for (let i = 0; i < 10; i++) {\n      this.available.push(this.factory());\n    }\n  }\n\n  acquire(): T {\n    let obj = this.available.pop();\n\n    if (!obj) {\n      obj = this.factory();\n    }\n\n    this.inUse.add(obj);\n    return obj;\n  }\n\n  release(obj: T): void {\n    if (!this.inUse.has(obj)) return;\n\n    this.inUse.delete(obj);\n    this.reset(obj);\n\n    if (this.available.length < this.maxSize) {\n      this.available.push(obj);\n    }\n  }\n}\n\n// Usage example\nconst bufferPool = new ObjectPool(\n  () => Buffer.allocUnsafe(1024 * 1024), // 1MB buffers\n  (buffer) => buffer.fill(0),\n  50\n);\n"})}),"\n",(0,i.jsx)(n.h3,{id:"3-memory-leak-prevention",children:"3. Memory Leak Prevention"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-typescript",children:"class LeakPrevention {\n  private subscriptions = new Map<string, () => void>();\n  private timers = new Map<string, NodeJS.Timer>();\n  private intervals = new Map<string, NodeJS.Timer>();\n\n  // Managed event subscription\n  on(emitter: EventEmitter, event: string, handler: Function): void {\n    const key = `${emitter.constructor.name}:${event}`;\n\n    // Remove existing subscription\n    this.off(key);\n\n    // Add new subscription\n    emitter.on(event, handler);\n    this.subscriptions.set(key, () => emitter.off(event, handler));\n  }\n\n  off(key: string): void {\n    const unsubscribe = this.subscriptions.get(key);\n    if (unsubscribe) {\n      unsubscribe();\n      this.subscriptions.delete(key);\n    }\n  }\n\n  // Managed timers\n  setTimeout(fn: Function, delay: number, key: string): void {\n    this.clearTimeout(key);\n\n    const timer = setTimeout(() => {\n      fn();\n      this.timers.delete(key);\n    }, delay);\n\n    this.timers.set(key, timer);\n  }\n\n  clearTimeout(key: string): void {\n    const timer = this.timers.get(key);\n    if (timer) {\n      clearTimeout(timer);\n      this.timers.delete(key);\n    }\n  }\n\n  // Cleanup all resources\n  cleanup(): void {\n    // Clear all subscriptions\n    for (const unsubscribe of this.subscriptions.values()) {\n      unsubscribe();\n    }\n    this.subscriptions.clear();\n\n    // Clear all timers\n    for (const timer of this.timers.values()) {\n      clearTimeout(timer);\n    }\n    this.timers.clear();\n\n    // Clear all intervals\n    for (const interval of this.intervals.values()) {\n      clearInterval(interval);\n    }\n    this.intervals.clear();\n  }\n}\n"})}),"\n",(0,i.jsx)(n.h2,{id:"scalability-patterns",children:"Scalability Patterns"}),"\n",(0,i.jsx)(n.h3,{id:"1-horizontal-scaling",children:"1. Horizontal Scaling"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-typescript",children:"class ClusterManager {\n  private workers: Worker[] = [];\n  private queue = new Queue<Task>();\n\n  async start(workerCount: number = os.cpus().length): Promise<void> {\n    // Spawn workers\n    for (let i = 0; i < workerCount; i++) {\n      const worker = new Worker('./worker.js');\n\n      worker.on('message', (result) => {\n        this.handleWorkerResult(worker, result);\n      });\n\n      worker.on('error', (error) => {\n        this.handleWorkerError(worker, error);\n      });\n\n      this.workers.push(worker);\n    }\n\n    // Start distributing work\n    this.distributeWork();\n  }\n\n  private async distributeWork(): Promise<void> {\n    while (true) {\n      const task = await this.queue.dequeue();\n      const worker = this.getAvailableWorker();\n\n      if (worker) {\n        worker.postMessage(task);\n      } else {\n        // Re-queue if no workers available\n        await this.queue.enqueue(task);\n        await sleep(100);\n      }\n    }\n  }\n\n  private getAvailableWorker(): Worker | null {\n    // Simple round-robin with availability check\n    return this.workers.find((w) => !w.busy) || null;\n  }\n}\n"})}),"\n",(0,i.jsx)(n.h3,{id:"2-load-balancing",children:"2. Load Balancing"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-typescript",children:"class LoadBalancer {\n  private endpoints: Endpoint[] = [];\n  private currentIndex = 0;\n\n  addEndpoint(endpoint: Endpoint): void {\n    endpoint.healthCheck = new HealthCheck(endpoint);\n    this.endpoints.push(endpoint);\n  }\n\n  async request(path: string, options?: RequestOptions): Promise<any> {\n    const maxRetries = 3;\n    let lastError: Error | null = null;\n\n    for (let i = 0; i < maxRetries; i++) {\n      const endpoint = this.selectEndpoint();\n\n      if (!endpoint) {\n        throw new Error('No healthy endpoints available');\n      }\n\n      try {\n        const response = await endpoint.request(path, options);\n        endpoint.recordSuccess();\n        return response;\n      } catch (error) {\n        endpoint.recordFailure();\n        lastError = error;\n\n        // Try next endpoint\n        continue;\n      }\n    }\n\n    throw lastError || new Error('All retries failed');\n  }\n\n  private selectEndpoint(): Endpoint | null {\n    // Weighted round-robin based on health\n    const healthyEndpoints = this.endpoints.filter((e) => e.isHealthy());\n\n    if (healthyEndpoints.length === 0) return null;\n\n    // Calculate weights based on response times\n    const weights = healthyEndpoints.map((e) => 1 / e.avgResponseTime);\n    const totalWeight = weights.reduce((a, b) => a + b, 0);\n\n    let random = Math.random() * totalWeight;\n\n    for (let i = 0; i < healthyEndpoints.length; i++) {\n      random -= weights[i];\n      if (random <= 0) {\n        return healthyEndpoints[i];\n      }\n    }\n\n    return healthyEndpoints[0];\n  }\n}\n"})}),"\n",(0,i.jsx)(n.h3,{id:"3-message-queue-integration",children:"3. Message Queue Integration"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-typescript",children:"class MessageQueueScaler {\n  private consumers: Consumer[] = [];\n\n  async autoScale(minConsumers: number = 1, maxConsumers: number = 10): Promise<void> {\n    setInterval(async () => {\n      const queueDepth = await this.getQueueDepth();\n      const avgProcessingTime = await this.getAvgProcessingTime();\n\n      // Calculate desired consumer count\n      const targetThroughput = queueDepth / 60; // Process queue in 1 minute\n      const currentThroughput = this.consumers.length / avgProcessingTime;\n\n      let desiredConsumers = Math.ceil(\n        this.consumers.length * (targetThroughput / currentThroughput)\n      );\n\n      // Apply bounds\n      desiredConsumers = Math.max(minConsumers, Math.min(maxConsumers, desiredConsumers));\n\n      // Scale up or down\n      if (desiredConsumers > this.consumers.length) {\n        await this.scaleUp(desiredConsumers - this.consumers.length);\n      } else if (desiredConsumers < this.consumers.length) {\n        await this.scaleDown(this.consumers.length - desiredConsumers);\n      }\n    }, 30000); // Check every 30 seconds\n  }\n\n  private async scaleUp(count: number): Promise<void> {\n    for (let i = 0; i < count; i++) {\n      const consumer = new Consumer({\n        queue: this.queue,\n        handler: this.messageHandler,\n      });\n\n      await consumer.start();\n      this.consumers.push(consumer);\n    }\n  }\n\n  private async scaleDown(count: number): Promise<void> {\n    // Gracefully stop consumers\n    const toStop = this.consumers.splice(-count);\n\n    await Promise.all(toStop.map((consumer) => consumer.stop()));\n  }\n}\n"})}),"\n",(0,i.jsx)(n.h2,{id:"database-performance",children:"Database Performance"}),"\n",(0,i.jsx)(n.h3,{id:"1-connection-pooling",children:"1. Connection Pooling"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-typescript",children:"class DatabasePool {\n  private pool: Pool;\n\n  constructor(config: PoolConfig) {\n    this.pool = new Pool({\n      ...config,\n\n      // Performance optimizations\n      max: 20, // Max connections\n      idleTimeoutMillis: 30000, // Close idle connections\n      connectionTimeoutMillis: 2000, // Connection timeout\n\n      // Connection lifecycle\n      async beforeConnect(config) {\n        // Pre-connection setup\n      },\n\n      async afterConnect(connection) {\n        // Post-connection optimization\n        await connection.query('SET statement_timeout = 30000');\n        await connection.query('SET lock_timeout = 10000');\n      },\n    });\n\n    // Monitor pool health\n    this.pool.on('error', (err, client) => {\n      console.error('Unexpected error on idle client', err);\n    });\n  }\n\n  async query(text: string, params?: any[]): Promise<any> {\n    const start = Date.now();\n\n    try {\n      const result = await this.pool.query(text, params);\n\n      const duration = Date.now() - start;\n      if (duration > 1000) {\n        console.warn(`Slow query (${duration}ms): ${text}`);\n      }\n\n      return result;\n    } catch (error) {\n      console.error('Query error:', error);\n      throw error;\n    }\n  }\n}\n"})}),"\n",(0,i.jsx)(n.h3,{id:"2-query-optimization-1",children:"2. Query Optimization"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-typescript",children:"class QueryOptimizer {\n  // Use EXPLAIN to analyze queries\n  async analyzeQuery(sql: string): Promise<QueryPlan> {\n    const result = await this.db.query(`EXPLAIN (ANALYZE, BUFFERS) ${sql}`);\n    return this.parseQueryPlan(result.rows);\n  }\n\n  // Optimize common query patterns\n  async getRecentMessages(userId: string, limit: number = 20): Promise<Message[]> {\n    // Use covering index\n    return await this.db.query(\n      `\n      SELECT id, content, timestamp, metadata\n      FROM messages\n      WHERE userId = $1\n        AND timestamp > NOW() - INTERVAL '7 days'\n      ORDER BY timestamp DESC\n      LIMIT $2\n    `,\n      [userId, limit]\n    );\n  }\n\n  // Batch operations\n  async batchInsert(records: any[]): Promise<void> {\n    const values: any[] = [];\n    const placeholders: string[] = [];\n\n    records.forEach((record, i) => {\n      const base = i * 4;\n      placeholders.push(`($${base + 1}, $${base + 2}, $${base + 3}, $${base + 4})`);\n      values.push(record.id, record.content, record.userId, record.timestamp);\n    });\n\n    await this.db.query(\n      `\n      INSERT INTO messages (id, content, userId, timestamp)\n      VALUES ${placeholders.join(', ')}\n      ON CONFLICT (id) DO NOTHING\n    `,\n      values\n    );\n  }\n}\n"})}),"\n",(0,i.jsx)(n.h2,{id:"network-optimization",children:"Network Optimization"}),"\n",(0,i.jsx)(n.h3,{id:"1-http2-and-compression",children:"1. HTTP/2 and Compression"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-typescript",children:"class OptimizedServer {\n  createServer(): void {\n    const server = http2.createSecureServer({\n      key: fs.readFileSync('key.pem'),\n      cert: fs.readFileSync('cert.pem'),\n\n      // HTTP/2 settings\n      settings: {\n        headerTableSize: 4096,\n        enablePush: true,\n        maxConcurrentStreams: 100,\n        initialWindowSize: 65535,\n        maxFrameSize: 16384,\n        maxHeaderListSize: 8192,\n      },\n    });\n\n    // Enable compression\n    server.on('stream', (stream, headers) => {\n      const acceptEncoding = headers['accept-encoding'] || '';\n\n      let contentEncoding = 'identity';\n      if (acceptEncoding.includes('br')) {\n        contentEncoding = 'br';\n        stream.respond({\n          'content-encoding': contentEncoding,\n          ':status': 200,\n        });\n        stream = stream.pipe(zlib.createBrotliCompress());\n      } else if (acceptEncoding.includes('gzip')) {\n        contentEncoding = 'gzip';\n        stream.respond({\n          'content-encoding': contentEncoding,\n          ':status': 200,\n        });\n        stream = stream.pipe(zlib.createGzip());\n      }\n\n      // Handle request...\n    });\n  }\n}\n"})}),"\n",(0,i.jsx)(n.h3,{id:"2-websocket-optimization",children:"2. WebSocket Optimization"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-typescript",children:"class OptimizedWebSocket {\n  private ws: WebSocket;\n  private messageQueue: any[] = [];\n  private sending = false;\n\n  constructor(url: string) {\n    this.ws = new WebSocket(url, {\n      perMessageDeflate: {\n        zlibDeflateOptions: {\n          level: zlib.Z_BEST_COMPRESSION,\n        },\n        threshold: 1024, // Compress messages > 1KB\n      },\n    });\n\n    this.setupBatching();\n  }\n\n  private setupBatching(): void {\n    // Batch small messages\n    setInterval(() => {\n      if (this.messageQueue.length > 0 && !this.sending) {\n        this.flushMessageQueue();\n      }\n    }, 50); // 50ms batching window\n  }\n\n  send(message: any): void {\n    this.messageQueue.push(message);\n\n    // Send immediately if queue is large\n    if (this.messageQueue.length >= 10) {\n      this.flushMessageQueue();\n    }\n  }\n\n  private async flushMessageQueue(): Promise<void> {\n    if (this.sending || this.messageQueue.length === 0) return;\n\n    this.sending = true;\n    const messages = this.messageQueue.splice(0);\n\n    try {\n      // Send as batch\n      await this.ws.send(\n        JSON.stringify({\n          type: 'batch',\n          messages,\n        })\n      );\n    } finally {\n      this.sending = false;\n    }\n  }\n}\n"})}),"\n",(0,i.jsx)(n.h2,{id:"cost-optimization",children:"Cost Optimization"}),"\n",(0,i.jsx)(n.h3,{id:"1-api-call-optimization",children:"1. API Call Optimization"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-typescript",children:"class APIOptimizer {\n  private cache = new Map<string, CachedResponse>();\n  private rateLimiter = new RateLimiter();\n\n  async callAPI(endpoint: string, params: any, options: APIOptions = {}): Promise<any> {\n    // Check cache first\n    const cacheKey = this.getCacheKey(endpoint, params);\n    const cached = this.cache.get(cacheKey);\n\n    if (cached && !this.isExpired(cached)) {\n      return cached.data;\n    }\n\n    // Rate limit check\n    await this.rateLimiter.acquire(endpoint);\n\n    try {\n      // Make API call\n      const response = await this.makeRequest(endpoint, params);\n\n      // Cache if configured\n      if (options.cache) {\n        this.cache.set(cacheKey, {\n          data: response,\n          timestamp: Date.now(),\n          ttl: options.cacheTTL || 300000, // 5 minutes\n        });\n      }\n\n      return response;\n    } finally {\n      this.rateLimiter.release(endpoint);\n    }\n  }\n\n  // Batch multiple API calls\n  async batchCall(requests: APIRequest[]): Promise<any[]> {\n    // Group by endpoint\n    const grouped = this.groupByEndpoint(requests);\n\n    // Execute batched calls\n    const results = await Promise.all(\n      Object.entries(grouped).map(([endpoint, batch]) => this.executeBatch(endpoint, batch))\n    );\n\n    // Flatten and reorder results\n    return this.reorderResults(requests, results.flat());\n  }\n}\n"})}),"\n",(0,i.jsx)(n.h3,{id:"2-resource-usage-optimization",children:"2. Resource Usage Optimization"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-typescript",children:"class ResourceOptimizer {\n  // Optimize model usage\n  async selectModel(task: Task): Promise<string> {\n    // Use smaller models for simple tasks\n    if (task.complexity === 'low') {\n      return 'gpt-3.5-turbo';\n    }\n\n    // Use larger models only when needed\n    if (task.complexity === 'high' || task.requiresReasoning) {\n      return 'gpt-4';\n    }\n\n    // Default to medium model\n    return 'gpt-3.5-turbo-16k';\n  }\n\n  // Optimize token usage\n  optimizePrompt(prompt: string, maxTokens: number): string {\n    // Remove unnecessary whitespace\n    let optimized = prompt.trim().replace(/\\s+/g, ' ');\n\n    // Truncate if too long\n    const estimatedTokens = optimized.length / 4; // Rough estimate\n    if (estimatedTokens > maxTokens * 0.8) {\n      optimized = this.intelligentTruncate(optimized, maxTokens * 0.8);\n    }\n\n    return optimized;\n  }\n\n  // Optimize storage\n  async compressOldData(): Promise<void> {\n    const oldData = await this.getDataOlderThan(30); // 30 days\n\n    for (const record of oldData) {\n      // Compress large text fields\n      if (record.content.length > 1000) {\n        record.compressedContent = await compress(record.content);\n        delete record.content;\n      }\n\n      // Remove unnecessary fields\n      delete record.tempData;\n      delete record.debugInfo;\n\n      await this.updateRecord(record);\n    }\n  }\n}\n"})}),"\n",(0,i.jsx)(n.h2,{id:"monitoring-and-profiling",children:"Monitoring and Profiling"}),"\n",(0,i.jsx)(n.h3,{id:"1-performance-profiling",children:"1. Performance Profiling"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-typescript",children:"class Profiler {\n  private profiles = new Map<string, Profile>();\n\n  startProfile(name: string): () => void {\n    const start = process.hrtime.bigint();\n    const startMemory = process.memoryUsage();\n\n    return () => {\n      const end = process.hrtime.bigint();\n      const endMemory = process.memoryUsage();\n\n      const profile: Profile = {\n        name,\n        duration: Number(end - start) / 1_000_000, // Convert to ms\n        memoryDelta: {\n          heapUsed: endMemory.heapUsed - startMemory.heapUsed,\n          external: endMemory.external - startMemory.external,\n        },\n        timestamp: new Date(),\n      };\n\n      this.addProfile(name, profile);\n    };\n  }\n\n  private addProfile(name: string, profile: Profile): void {\n    if (!this.profiles.has(name)) {\n      this.profiles.set(name, profile);\n    } else {\n      // Update running average\n      const existing = this.profiles.get(name)!;\n      existing.duration = (existing.duration + profile.duration) / 2;\n      existing.calls = (existing.calls || 1) + 1;\n    }\n  }\n\n  getReport(): ProfileReport {\n    const report: ProfileReport = {\n      profiles: Array.from(this.profiles.values()),\n      summary: {\n        totalDuration: 0,\n        totalMemory: 0,\n        slowestOperation: null,\n        mostFrequent: null,\n      },\n    };\n\n    // Calculate summary statistics\n    for (const profile of report.profiles) {\n      report.summary.totalDuration += profile.duration;\n\n      if (\n        !report.summary.slowestOperation ||\n        profile.duration > report.summary.slowestOperation.duration\n      ) {\n        report.summary.slowestOperation = profile;\n      }\n    }\n\n    return report;\n  }\n}\n"})}),"\n",(0,i.jsx)(n.h3,{id:"2-real-time-monitoring",children:"2. Real-time Monitoring"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-typescript",children:"class RealtimeMonitor {\n  private metrics = new MetricsCollector();\n  private alerts = new AlertManager();\n\n  start(): void {\n    // Collect metrics\n    this.collectMetrics();\n\n    // Setup alerts\n    this.setupAlerts();\n\n    // Export metrics\n    this.exportMetrics();\n  }\n\n  private collectMetrics(): void {\n    // Response time histogram\n    runtime.on('request:complete', ({ duration }) => {\n      this.metrics.histogram('response_time', duration);\n    });\n\n    // Active connections gauge\n    setInterval(() => {\n      this.metrics.gauge('active_connections', runtime.getActiveConnections());\n    }, 5000);\n\n    // Error rate counter\n    runtime.on('error', ({ type }) => {\n      this.metrics.counter('errors', { type });\n    });\n  }\n\n  private setupAlerts(): void {\n    // High response time alert\n    this.alerts.create({\n      name: 'high_response_time',\n      condition: () => this.metrics.percentile('response_time', 95) > 1000,\n      message: 'P95 response time > 1s',\n      cooldown: 5 * 60 * 1000, // 5 minutes\n    });\n\n    // Memory usage alert\n    this.alerts.create({\n      name: 'high_memory',\n      condition: () => process.memoryUsage().heapUsed > 1024 * 1024 * 1024,\n      message: 'Heap usage > 1GB',\n      cooldown: 10 * 60 * 1000,\n    });\n  }\n}\n"})}),"\n",(0,i.jsx)(n.h2,{id:"best-practices-summary",children:"Best Practices Summary"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Measure First"}),": Profile before optimizing"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Cache Strategically"}),": Cache expensive operations with appropriate TTLs"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Batch Operations"}),": Group similar operations together"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Async by Default"}),": Use async/await and parallel processing"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Monitor Continuously"}),": Track performance metrics in production"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Optimize Queries"}),": Use indexes and analyze query plans"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Manage Resources"}),": Implement pooling and cleanup"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Scale Horizontally"}),": Design for distributed systems"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Compress Data"}),": Use compression for network and storage"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Cost Awareness"}),": Monitor and optimize API usage"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"related-documentation",children:"Related Documentation"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"/docs/technical/advanced/best-practices",children:"Best Practices"})," - General development best practices"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"/docs/technical/architecture/state-management",children:"State Management"})," - Efficient state handling"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"/docs/technical/architecture/memory-system",children:"Memory System"})," - Memory optimization techniques"]}),"\n"]})]})}function d(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(m,{...e})}):m(e)}},71184:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>o});var s=t(14041);const i={},r=s.createContext(i);function a(e){const n=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),s.createElement(r.Provider,{value:n},e.children)}}}]);