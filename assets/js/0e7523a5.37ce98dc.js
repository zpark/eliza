"use strict";(self.webpackChunk_elizaos_docs=self.webpackChunk_elizaos_docs||[]).push([[74077],{71184:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>o});var i=t(14041);const s={},r=i.createContext(s);function a(e){const n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),i.createElement(r.Provider,{value:n},e.children)}},80060:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>d,frontMatter:()=>a,metadata:()=>i,toc:()=>m});const i=JSON.parse('{"id":"technical/architecture/memory-system","title":"Memory System","description":"Deep dive into ElizaOS memory architecture and implementation","source":"@site/docs/technical/architecture/memory-system.md","sourceDirName":"technical/architecture","slug":"/technical/architecture/memory-system","permalink":"/docs/technical/architecture/memory-system","draft":false,"unlisted":false,"editUrl":"https://github.com/elizaos/eliza/tree/develop/packages/docs/docs/technical/architecture/memory-system.md","tags":[],"version":"current","lastUpdatedBy":"SYMBaiEX","lastUpdatedAt":1751753321000,"frontMatter":{"title":"Memory System","description":"Deep dive into ElizaOS memory architecture and implementation"},"sidebar":"technicalSidebar","previous":{"title":"\ud83d\udcbe State Management","permalink":"/docs/technical/architecture/state-management"},"next":{"title":"\ud83d\udd0c Plugin System","permalink":"/docs/technical/architecture/plugin-system"}}');var s=t(31085),r=t(71184);const a={title:"Memory System",description:"Deep dive into ElizaOS memory architecture and implementation"},o="Memory System",c={},m=[{value:"Overview",id:"overview",level:2},{value:"Architecture",id:"architecture",level:2},{value:"Memory Types",id:"memory-types",level:2},{value:"1. Short-term Memory",id:"1-short-term-memory",level:3},{value:"2. Long-term Memory",id:"2-long-term-memory",level:3},{value:"3. Episodic Memory",id:"3-episodic-memory",level:3},{value:"4. Semantic Memory",id:"4-semantic-memory",level:3},{value:"Memory Processing Pipeline",id:"memory-processing-pipeline",level:2},{value:"1. Ingestion",id:"1-ingestion",level:3},{value:"2. Feature Extraction",id:"2-feature-extraction",level:3},{value:"3. Importance Scoring",id:"3-importance-scoring",level:3},{value:"Memory Retrieval",id:"memory-retrieval",level:2},{value:"1. Semantic Search",id:"1-semantic-search",level:3},{value:"2. Context-Aware Retrieval",id:"2-context-aware-retrieval",level:3},{value:"3. Memory Consolidation",id:"3-memory-consolidation",level:3},{value:"Vector Embeddings",id:"vector-embeddings",level:2},{value:"1. Embedding Generation",id:"1-embedding-generation",level:3},{value:"2. Similarity Calculation",id:"2-similarity-calculation",level:3},{value:"Memory Management",id:"memory-management",level:2},{value:"1. Storage Optimization",id:"1-storage-optimization",level:3},{value:"2. Memory Decay",id:"2-memory-decay",level:3},{value:"Integration with Agent Runtime",id:"integration-with-agent-runtime",level:2},{value:"1. Memory Middleware",id:"1-memory-middleware",level:3},{value:"2. Memory-Enhanced Responses",id:"2-memory-enhanced-responses",level:3},{value:"Performance Considerations",id:"performance-considerations",level:2},{value:"1. Indexing Strategies",id:"1-indexing-strategies",level:3},{value:"2. Caching Layer",id:"2-caching-layer",level:3},{value:"3. Batch Processing",id:"3-batch-processing",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"Related Documentation",id:"related-documentation",level:2}];function l(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"memory-system",children:"Memory System"})}),"\n",(0,s.jsx)(n.p,{children:"The ElizaOS memory system provides agents with sophisticated memory capabilities, enabling context-aware conversations and long-term relationship building."}),"\n",(0,s.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(n.p,{children:"The memory system in ElizaOS is designed to:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Store and retrieve conversation history"}),"\n",(0,s.jsx)(n.li,{children:"Build knowledge graphs from interactions"}),"\n",(0,s.jsx)(n.li,{children:"Enable semantic search across memories"}),"\n",(0,s.jsx)(n.li,{children:"Support different memory types and importance levels"}),"\n",(0,s.jsx)(n.li,{children:"Scale efficiently with vector embeddings"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"architecture",children:"Architecture"}),"\n",(0,s.jsx)(n.mermaid,{value:'graph TB\n    subgraph "Memory Pipeline"\n        Input[Message Input] --\x3e Proc[Memory Processor]\n        Proc --\x3e Extract[Feature Extraction]\n        Extract --\x3e Embed[Embedding Generation]\n        Embed --\x3e Store[Vector Store]\n\n        Store --\x3e Short[Short-term Memory]\n        Store --\x3e Long[Long-term Memory]\n        Store --\x3e Know[Knowledge Graph]\n    end\n\n    subgraph "Retrieval"\n        Query[Query] --\x3e Search[Semantic Search]\n        Search --\x3e Rank[Relevance Ranking]\n        Rank --\x3e Filter[Context Filtering]\n        Filter --\x3e Results[Memory Results]\n    end'}),"\n",(0,s.jsx)(n.h2,{id:"memory-types",children:"Memory Types"}),"\n",(0,s.jsx)(n.h3,{id:"1-short-term-memory",children:"1. Short-term Memory"}),"\n",(0,s.jsx)(n.p,{children:"Short-term memory stores recent conversation context:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"interface ShortTermMemory {\n  messages: Message[]; // Recent messages\n  context: ConversationContext; // Current context\n  maxSize: number; // Typically 10-50 messages\n  ttl: number; // Time to live\n}\n\n// Implementation\nclass ShortTermMemoryManager {\n  private memories = new Map<string, Message[]>();\n\n  addMessage(conversationId: string, message: Message): void {\n    const messages = this.memories.get(conversationId) || [];\n    messages.push(message);\n\n    // Keep only recent messages\n    if (messages.length > this.maxSize) {\n      messages.shift();\n    }\n\n    this.memories.set(conversationId, messages);\n  }\n\n  getRecentContext(conversationId: string): Message[] {\n    return this.memories.get(conversationId) || [];\n  }\n}\n"})}),"\n",(0,s.jsx)(n.h3,{id:"2-long-term-memory",children:"2. Long-term Memory"}),"\n",(0,s.jsx)(n.p,{children:"Long-term memory persists important information:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"interface LongTermMemory {\n  id: string;\n  content: string;\n  embedding: number[]; // Vector embedding\n  metadata: {\n    userId: string;\n    timestamp: Date;\n    importance: number; // 0-1 score\n    topics: string[];\n    entities: string[];\n    sentiment: number;\n  };\n}\n\n// Storage implementation\nclass LongTermMemoryStore {\n  async store(memory: Memory): Promise<void> {\n    // Generate embedding\n    const embedding = await this.generateEmbedding(memory.content);\n\n    // Store in vector database\n    await this.vectorDB.upsert({\n      id: memory.id,\n      vector: embedding,\n      metadata: memory.metadata,\n    });\n\n    // Store in relational database for structured queries\n    await this.db.memories.create({\n      data: memory,\n    });\n  }\n}\n"})}),"\n",(0,s.jsx)(n.h3,{id:"3-episodic-memory",children:"3. Episodic Memory"}),"\n",(0,s.jsx)(n.p,{children:"Episodic memories capture significant events:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"interface EpisodicMemory {\n  id: string;\n  event: string;\n  participants: string[];\n  location?: string;\n  timestamp: Date;\n  emotionalValence: number;\n  significance: number;\n  relatedMemories: string[];\n}\n\n// Event detection\nclass EpisodeDetector {\n  async detectEpisode(messages: Message[]): Promise<EpisodicMemory | null> {\n    // Analyze message sequence for significant events\n    const features = await this.extractFeatures(messages);\n\n    if (features.significance > this.threshold) {\n      return {\n        id: generateId(),\n        event: features.eventSummary,\n        participants: features.entities.filter((e) => e.type === 'person'),\n        timestamp: new Date(),\n        emotionalValence: features.sentiment,\n        significance: features.significance,\n        relatedMemories: await this.findRelated(features),\n      };\n    }\n\n    return null;\n  }\n}\n"})}),"\n",(0,s.jsx)(n.h3,{id:"4-semantic-memory",children:"4. Semantic Memory"}),"\n",(0,s.jsx)(n.p,{children:"Semantic memory stores facts and knowledge:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:'interface SemanticMemory {\n  concept: string;\n  relations: Relation[];\n  properties: Property[];\n  confidence: number;\n  sources: string[]; // Memory IDs that support this fact\n}\n\ninterface Relation {\n  type: string; // "is-a", "has", "located-in", etc.\n  target: string;\n  confidence: number;\n}\n\n// Knowledge graph builder\nclass KnowledgeGraphBuilder {\n  async extractKnowledge(memory: Memory): Promise<SemanticMemory[]> {\n    const facts = [];\n\n    // Extract entities and relations\n    const extraction = await this.nlp.extract(memory.content);\n\n    for (const triple of extraction.triples) {\n      facts.push({\n        concept: triple.subject,\n        relations: [\n          {\n            type: triple.predicate,\n            target: triple.object,\n            confidence: triple.confidence,\n          },\n        ],\n        properties: triple.properties,\n        confidence: triple.confidence,\n        sources: [memory.id],\n      });\n    }\n\n    return facts;\n  }\n}\n'})}),"\n",(0,s.jsx)(n.h2,{id:"memory-processing-pipeline",children:"Memory Processing Pipeline"}),"\n",(0,s.jsx)(n.h3,{id:"1-ingestion",children:"1. Ingestion"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"class MemoryIngestion {\n  async process(message: Message): Promise<void> {\n    // 1. Preprocess\n    const processed = await this.preprocess(message);\n\n    // 2. Extract features\n    const features = await this.extractFeatures(processed);\n\n    // 3. Generate embeddings\n    const embedding = await this.generateEmbedding(processed);\n\n    // 4. Calculate importance\n    const importance = await this.calculateImportance(features);\n\n    // 5. Store memory\n    await this.store({\n      content: processed,\n      embedding,\n      features,\n      importance,\n      metadata: {\n        entityId: message.entityId,\n        timestamp: new Date(),\n        roomId: message.roomId,\n      },\n    });\n  }\n}\n"})}),"\n",(0,s.jsx)(n.h3,{id:"2-feature-extraction",children:"2. Feature Extraction"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"interface MemoryFeatures {\n  entities: Entity[];\n  topics: string[];\n  sentiment: number;\n  emotions: Emotion[];\n  intentions: string[];\n  keyPhrases: string[];\n}\n\nclass FeatureExtractor {\n  async extract(text: string): Promise<MemoryFeatures> {\n    const [entities, topics, sentiment, emotions] = await Promise.all([\n      this.extractEntities(text),\n      this.extractTopics(text),\n      this.analyzeSentiment(text),\n      this.detectEmotions(text),\n    ]);\n\n    return {\n      entities,\n      topics,\n      sentiment,\n      emotions,\n      intentions: await this.classifyIntentions(text),\n      keyPhrases: await this.extractKeyPhrases(text),\n    };\n  }\n}\n"})}),"\n",(0,s.jsx)(n.h3,{id:"3-importance-scoring",children:"3. Importance Scoring"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"class ImportanceScorer {\n  async calculate(memory: Memory, features: MemoryFeatures): Promise<number> {\n    const weights = {\n      emotional: 0.3,\n      novelty: 0.2,\n      personal: 0.2,\n      factual: 0.15,\n      recency: 0.15,\n    };\n\n    const scores = {\n      emotional: this.emotionalScore(features.emotions),\n      novelty: await this.noveltyScore(memory, features),\n      personal: this.personalRelevance(features.entities),\n      factual: this.factualImportance(features),\n      recency: this.recencyScore(memory.timestamp),\n    };\n\n    // Weighted sum\n    return Object.entries(scores).reduce((total, [key, score]) => total + score * weights[key], 0);\n  }\n}\n"})}),"\n",(0,s.jsx)(n.h2,{id:"memory-retrieval",children:"Memory Retrieval"}),"\n",(0,s.jsx)(n.h3,{id:"1-semantic-search",children:"1. Semantic Search"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"class MemoryRetrieval {\n  async search(query: string, options: SearchOptions): Promise<Memory[]> {\n    // Generate query embedding\n    const queryEmbedding = await this.generateEmbedding(query);\n\n    // Vector similarity search\n    const similar = await this.vectorDB.search({\n      vector: queryEmbedding,\n      topK: options.limit || 10,\n      filter: this.buildFilter(options),\n    });\n\n    // Re-rank by relevance\n    const reranked = await this.rerank(query, similar);\n\n    // Apply context filtering\n    return this.filterByContext(reranked, options.context);\n  }\n\n  private buildFilter(options: SearchOptions): any {\n    const filter = {};\n\n    if (options.userId) {\n      filter['metadata.userId'] = options.userId;\n    }\n\n    if (options.timeRange) {\n      filter['metadata.timestamp'] = {\n        $gte: options.timeRange.start,\n        $lte: options.timeRange.end,\n      };\n    }\n\n    if (options.minImportance) {\n      filter['metadata.importance'] = {\n        $gte: options.minImportance,\n      };\n    }\n\n    return filter;\n  }\n}\n"})}),"\n",(0,s.jsx)(n.h3,{id:"2-context-aware-retrieval",children:"2. Context-Aware Retrieval"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"class ContextualRetrieval {\n  async retrieve(context: ConversationContext, limit: number = 5): Promise<Memory[]> {\n    // Multi-stage retrieval\n    const stages = [\n      // Recent context\n      this.getRecentMemories(context.conversationId, 5),\n\n      // Topically related\n      this.getTopicalMemories(context.topics, 10),\n\n      // Entity-based\n      this.getEntityMemories(context.entities, 10),\n\n      // Emotionally similar\n      this.getEmotionalMemories(context.emotionalState, 5),\n    ];\n\n    const results = await Promise.all(stages);\n\n    // Merge and deduplicate\n    const merged = this.mergeResults(results.flat());\n\n    // Score by relevance\n    const scored = await this.scoreRelevance(merged, context);\n\n    // Return top memories\n    return scored.slice(0, limit);\n  }\n}\n"})}),"\n",(0,s.jsx)(n.h3,{id:"3-memory-consolidation",children:"3. Memory Consolidation"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"class MemoryConsolidation {\n  async consolidate(): Promise<void> {\n    // Find similar memories\n    const clusters = await this.clusterMemories();\n\n    for (const cluster of clusters) {\n      // Skip if too small\n      if (cluster.size < this.minClusterSize) continue;\n\n      // Generate summary\n      const summary = await this.summarizeCluster(cluster);\n\n      // Create consolidated memory\n      const consolidated = {\n        type: 'consolidated',\n        content: summary,\n        sources: cluster.memories.map((m) => m.id),\n        importance: Math.max(...cluster.memories.map((m) => m.importance)),\n        timestamp: new Date(),\n      };\n\n      // Store consolidated memory\n      await this.store(consolidated);\n\n      // Optionally archive original memories\n      if (this.archiveOriginals) {\n        await this.archive(cluster.memories);\n      }\n    }\n  }\n}\n"})}),"\n",(0,s.jsx)(n.h2,{id:"vector-embeddings",children:"Vector Embeddings"}),"\n",(0,s.jsx)(n.h3,{id:"1-embedding-generation",children:"1. Embedding Generation"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"class EmbeddingGenerator {\n  async generate(text: string): Promise<number[]> {\n    // Preprocess text\n    const processed = this.preprocess(text);\n\n    // Generate embedding using model\n    const embedding = await this.model.embed(processed);\n\n    // Normalize\n    return this.normalize(embedding);\n  }\n\n  private preprocess(text: string): string {\n    // Remove unnecessary whitespace\n    let processed = text.trim().replace(/\\s+/g, ' ');\n\n    // Truncate if too long\n    if (processed.length > this.maxLength) {\n      processed = this.truncateSmartly(processed);\n    }\n\n    return processed;\n  }\n\n  private normalize(embedding: number[]): number[] {\n    const magnitude = Math.sqrt(embedding.reduce((sum, val) => sum + val * val, 0));\n\n    return embedding.map((val) => val / magnitude);\n  }\n}\n"})}),"\n",(0,s.jsx)(n.h3,{id:"2-similarity-calculation",children:"2. Similarity Calculation"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"class SimilarityCalculator {\n  cosineSimilarity(a: number[], b: number[]): number {\n    let dotProduct = 0;\n    let normA = 0;\n    let normB = 0;\n\n    for (let i = 0; i < a.length; i++) {\n      dotProduct += a[i] * b[i];\n      normA += a[i] * a[i];\n      normB += b[i] * b[i];\n    }\n\n    return dotProduct / (Math.sqrt(normA) * Math.sqrt(normB));\n  }\n\n  async findSimilar(\n    embedding: number[],\n    candidates: Memory[],\n    threshold: number = 0.7\n  ): Promise<SimilarMemory[]> {\n    const similarities = [];\n\n    for (const candidate of candidates) {\n      const similarity = this.cosineSimilarity(embedding, candidate.embedding);\n\n      if (similarity >= threshold) {\n        similarities.push({\n          memory: candidate,\n          similarity,\n        });\n      }\n    }\n\n    return similarities.sort((a, b) => b.similarity - a.similarity);\n  }\n}\n"})}),"\n",(0,s.jsx)(n.h2,{id:"memory-management",children:"Memory Management"}),"\n",(0,s.jsx)(n.h3,{id:"1-storage-optimization",children:"1. Storage Optimization"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"class MemoryOptimizer {\n  async optimize(): Promise<void> {\n    // Remove duplicates\n    await this.deduplicateMemories();\n\n    // Compress old memories\n    await this.compressOldMemories();\n\n    // Update importance scores\n    await this.recalculateImportance();\n\n    // Prune low-value memories\n    await this.pruneLowValueMemories();\n  }\n\n  private async deduplicateMemories(): Promise<void> {\n    const memories = await this.getAllMemories();\n    const seen = new Set<string>();\n    const duplicates = [];\n\n    for (const memory of memories) {\n      const hash = this.hashMemory(memory);\n\n      if (seen.has(hash)) {\n        duplicates.push(memory.id);\n      } else {\n        seen.add(hash);\n      }\n    }\n\n    await this.removeMemories(duplicates);\n  }\n}\n"})}),"\n",(0,s.jsx)(n.h3,{id:"2-memory-decay",children:"2. Memory Decay"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"class MemoryDecay {\n  async applyDecay(): Promise<void> {\n    const memories = await this.getDecayableMemories();\n\n    for (const memory of memories) {\n      // Calculate decay based on time and access\n      const decayFactor = this.calculateDecay(\n        memory.timestamp,\n        memory.lastAccessed,\n        memory.accessCount\n      );\n\n      // Update importance\n      memory.importance *= decayFactor;\n\n      // Remove if below threshold\n      if (memory.importance < this.removalThreshold) {\n        await this.removeMemory(memory.id);\n      } else {\n        await this.updateMemory(memory);\n      }\n    }\n  }\n\n  private calculateDecay(created: Date, lastAccessed: Date, accessCount: number): number {\n    const age = Date.now() - created.getTime();\n    const recency = Date.now() - lastAccessed.getTime();\n\n    // Forgetting curve\n    const timeFactor = Math.exp(-age / this.halfLife);\n\n    // Access bonus\n    const accessFactor = Math.log(accessCount + 1) / 10;\n\n    // Recency bonus\n    const recencyFactor = Math.exp(-recency / this.recencyWeight);\n\n    return Math.min(1, timeFactor + accessFactor + recencyFactor);\n  }\n}\n"})}),"\n",(0,s.jsx)(n.h2,{id:"integration-with-agent-runtime",children:"Integration with Agent Runtime"}),"\n",(0,s.jsx)(n.h3,{id:"1-memory-middleware",children:"1. Memory Middleware"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"class MemoryMiddleware {\n  async process(\n    message: Message,\n    context: Context,\n    next: () => Promise<Response>\n  ): Promise<Response> {\n    // Load relevant memories\n    const memories = await this.memoryManager.retrieve(context);\n\n    // Enhance context with memories\n    context.memories = memories;\n    context.memoryContext = this.buildMemoryContext(memories);\n\n    // Process message\n    const response = await next();\n\n    // Store new memory\n    await this.memoryManager.store({\n      message,\n      response,\n      context,\n    });\n\n    return response;\n  }\n}\n"})}),"\n",(0,s.jsx)(n.h3,{id:"2-memory-enhanced-responses",children:"2. Memory-Enhanced Responses"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"class MemoryEnhancedAgent {\n  async generateResponse(message: Message, memories: Memory[]): Promise<Response> {\n    // Build prompt with memory context\n    const prompt = this.buildPromptWithMemories(message, memories);\n\n    // Generate response\n    const response = await this.llm.generate(prompt);\n\n    // Ensure consistency with memories\n    const validated = await this.validateAgainstMemories(response, memories);\n\n    return validated;\n  }\n\n  private buildPromptWithMemories(message: Message, memories: Memory[]): string {\n    const relevantMemories = memories\n      .map((m) => `- ${m.content} (${m.metadata.timeAgo})`)\n      .join('\\n');\n\n    return `\nPrevious relevant conversations:\n${relevantMemories}\n\nCurrent message: ${message.content}\n\nGenerate a response that is consistent with the conversation history.\n    `;\n  }\n}\n"})}),"\n",(0,s.jsx)(n.h2,{id:"performance-considerations",children:"Performance Considerations"}),"\n",(0,s.jsx)(n.h3,{id:"1-indexing-strategies",children:"1. Indexing Strategies"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"// Compound indexes for efficient queries\nconst indexes = [\n  { userId: 1, timestamp: -1 },\n  { importance: -1, timestamp: -1 },\n  { 'metadata.topics': 1 },\n  { 'metadata.entities': 1 },\n];\n\n// Vector index for similarity search\nconst vectorIndex = {\n  type: 'ivfflat',\n  dimensions: 1536,\n  lists: 100,\n};\n"})}),"\n",(0,s.jsx)(n.h3,{id:"2-caching-layer",children:"2. Caching Layer"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"class MemoryCache {\n  private cache = new LRUCache<string, Memory[]>({\n    max: 1000,\n    ttl: 5 * 60 * 1000, // 5 minutes\n  });\n\n  async get(key: string): Promise<Memory[] | null> {\n    return this.cache.get(key) || null;\n  }\n\n  async set(key: string, memories: Memory[]): Promise<void> {\n    this.cache.set(key, memories);\n  }\n}\n"})}),"\n",(0,s.jsx)(n.h3,{id:"3-batch-processing",children:"3. Batch Processing"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"class BatchMemoryProcessor {\n  private queue: Memory[] = [];\n  private processing = false;\n\n  async add(memory: Memory): Promise<void> {\n    this.queue.push(memory);\n\n    if (!this.processing && this.queue.length >= this.batchSize) {\n      await this.processBatch();\n    }\n  }\n\n  private async processBatch(): Promise<void> {\n    this.processing = true;\n\n    const batch = this.queue.splice(0, this.batchSize);\n\n    // Generate embeddings in batch\n    const embeddings = await this.generateBatchEmbeddings(batch.map((m) => m.content));\n\n    // Store in batch\n    await this.batchStore(\n      batch.map((m, i) => ({\n        ...m,\n        embedding: embeddings[i],\n      }))\n    );\n\n    this.processing = false;\n  }\n}\n"})}),"\n",(0,s.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Memory Hygiene"}),": Regularly clean and consolidate memories"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Importance Scoring"}),": Use multi-factor importance calculation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Context Windows"}),": Limit memory context to maintain performance"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Privacy"}),": Implement memory access controls and data retention policies"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Embedding Quality"}),": Use appropriate models for your use case"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Monitoring"}),": Track memory system performance and usage"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"related-documentation",children:"Related Documentation"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"/docs/technical/architecture/state-management",children:"State Management"})," - Overall state architecture"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"/docs/technical/architecture/core-concepts",children:"Core Concepts"})," - Fundamental ElizaOS concepts"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"/docs/technical/advanced/performance",children:"Performance Guide"})," - Optimization techniques"]}),"\n"]})]})}function d(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(l,{...e})}):l(e)}}}]);