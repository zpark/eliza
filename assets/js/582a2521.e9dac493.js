"use strict";(self.webpackChunkeliza_docs=self.webpackChunkeliza_docs||[]).push([[9047],{9966:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>t,contentTitle:()=>r,default:()=>u,frontMatter:()=>d,metadata:()=>o,toc:()=>c});var s=i(4848),l=i(8453);const d={sidebar_position:1,title:"Fine-tuning"},r="Model Selection and Fine-tuning",o={id:"advanced/fine-tuning",title:"Fine-tuning",description:"Overview",source:"@site/docs/advanced/fine-tuning.md",sourceDirName:"advanced",slug:"/advanced/fine-tuning",permalink:"/eliza/docs/advanced/fine-tuning",draft:!1,unlisted:!1,editUrl:"https://github.com/ai16z/eliza/tree/main/docs/docs/advanced/fine-tuning.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1,title:"Fine-tuning"},sidebar:"tutorialSidebar",previous:{title:"Local Development",permalink:"/eliza/docs/guides/local-development"},next:{title:"Infrastructure",permalink:"/eliza/docs/advanced/infrastructure"}},t={},c=[{value:"Overview",id:"overview",level:2},{value:"Supported Models",id:"supported-models",level:2},{value:"Available Providers",id:"available-providers",level:3},{value:"Configuration Options",id:"configuration-options",level:2},{value:"Model Settings",id:"model-settings",level:3},{value:"Model Classes",id:"model-classes",level:3},{value:"Fine-tuning Guidelines",id:"fine-tuning-guidelines",level:2},{value:"1. Selecting the Right Model Size",id:"1-selecting-the-right-model-size",level:3},{value:"2. Optimizing Model Parameters",id:"2-optimizing-model-parameters",level:3},{value:"3. Embedding Configuration",id:"3-embedding-configuration",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"Common Issues and Solutions",id:"common-issues-and-solutions",level:2},{value:"Advanced Configuration",id:"advanced-configuration",level:2},{value:"Additional Resources",id:"additional-resources",level:2}];function a(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,l.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"model-selection-and-fine-tuning",children:"Model Selection and Fine-tuning"})}),"\n",(0,s.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(n.p,{children:"Eliza provides a flexible model selection and configuration system that supports multiple AI providers including OpenAI, Anthropic, Google, and various LLaMA implementations. This guide explains how to configure and fine-tune models for optimal performance in your use case."}),"\n",(0,s.jsx)(n.h2,{id:"supported-models",children:"Supported Models"}),"\n",(0,s.jsx)(n.h3,{id:"available-providers",children:"Available Providers"}),"\n",(0,s.jsx)(n.p,{children:"Eliza supports the following model providers:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"OpenAI"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Small: gpt-4o-mini"}),"\n",(0,s.jsx)(n.li,{children:"Medium: gpt-4o"}),"\n",(0,s.jsx)(n.li,{children:"Large: gpt-4o"}),"\n",(0,s.jsx)(n.li,{children:"Embeddings: text-embedding-3-small"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Anthropic"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Small: claude-3-haiku"}),"\n",(0,s.jsx)(n.li,{children:"Medium: claude-3.5-sonnet"}),"\n",(0,s.jsx)(n.li,{children:"Large: claude-3-opus"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Google (Gemini)"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Small: gemini-1.5-flash"}),"\n",(0,s.jsx)(n.li,{children:"Medium: gemini-1.5-flash"}),"\n",(0,s.jsx)(n.li,{children:"Large: gemini-1.5-pro"}),"\n",(0,s.jsx)(n.li,{children:"Embeddings: text-embedding-004"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"LLaMA Cloud"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Small: meta-llama/Llama-3.2-3B-Instruct-Turbo"}),"\n",(0,s.jsx)(n.li,{children:"Medium: meta-llama-3.1-8b-instruct"}),"\n",(0,s.jsx)(n.li,{children:"Large: meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo"}),"\n",(0,s.jsx)(n.li,{children:"Embeddings: togethercomputer/m2-bert-80M-32k-retrieval"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"LLaMA Local"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Various Hermes-3-Llama models optimized for local deployment"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"configuration-options",children:"Configuration Options"}),"\n",(0,s.jsx)(n.h3,{id:"model-settings",children:"Model Settings"}),"\n",(0,s.jsx)(n.p,{children:"Each model provider can be configured with the following parameters:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"settings: {\n    stop: [],                    // Stop sequences for text generation\n    maxInputTokens: 128000,      // Maximum input context length\n    maxOutputTokens: 8192,       // Maximum response length\n    frequency_penalty: 0.0,      // Penalize frequent tokens\n    presence_penalty: 0.0,       // Penalize repeated content\n    temperature: 0.3,            // Control randomness (0.0-1.0)\n}\n"})}),"\n",(0,s.jsx)(n.h3,{id:"model-classes",children:"Model Classes"}),"\n",(0,s.jsx)(n.p,{children:"Models are categorized into four classes:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"SMALL"}),": Optimized for speed and cost"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"MEDIUM"}),": Balanced performance and capability"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"LARGE"}),": Maximum capability for complex tasks"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"EMBEDDING"}),": Specialized for text embeddings"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"fine-tuning-guidelines",children:"Fine-tuning Guidelines"}),"\n",(0,s.jsx)(n.h3,{id:"1-selecting-the-right-model-size",children:"1. Selecting the Right Model Size"}),"\n",(0,s.jsx)(n.p,{children:"Choose your model class based on your requirements:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"SMALL Models"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Best for: Quick responses, simple tasks, cost-effective deployment"}),"\n",(0,s.jsx)(n.li,{children:"Example use cases: Basic chat, simple classifications"}),"\n",(0,s.jsxs)(n.li,{children:["Recommended: ",(0,s.jsx)(n.code,{children:"claude-3-haiku"})," or ",(0,s.jsx)(n.code,{children:"gemini-1.5-flash"})]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"MEDIUM Models"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Best for: General purpose applications, balanced performance"}),"\n",(0,s.jsx)(n.li,{children:"Example use cases: Content generation, complex analysis"}),"\n",(0,s.jsxs)(n.li,{children:["Recommended: ",(0,s.jsx)(n.code,{children:"claude-3.5-sonnet"})," or ",(0,s.jsx)(n.code,{children:"meta-llama-3.1-8b-instruct"})]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"LARGE Models"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Best for: Complex reasoning, specialized tasks"}),"\n",(0,s.jsx)(n.li,{children:"Example use cases: Code generation, detailed analysis"}),"\n",(0,s.jsxs)(n.li,{children:["Recommended: ",(0,s.jsx)(n.code,{children:"claude-3-opus"})," or ",(0,s.jsx)(n.code,{children:"Meta-Llama-3.1-405B"})]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"2-optimizing-model-parameters",children:"2. Optimizing Model Parameters"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"// Example configuration for different use cases\nconst chatConfig = {\n    temperature: 0.7,        // More creative responses\n    maxOutputTokens: 2048,   // Shorter, focused replies\n    presence_penalty: 0.6    // Encourage response variety\n};\n\nconst analysisConfig = {\n    temperature: 0.2,        // More deterministic responses\n    maxOutputTokens: 8192,   // Allow detailed analysis\n    presence_penalty: 0.0    // Maintain focused analysis\n};\n"})}),"\n",(0,s.jsx)(n.h3,{id:"3-embedding-configuration",children:"3. Embedding Configuration"}),"\n",(0,s.jsx)(n.p,{children:"Eliza includes a sophisticated embedding system that supports:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Automatic caching of embeddings"}),"\n",(0,s.jsx)(n.li,{children:"Provider-specific optimizations"}),"\n",(0,s.jsx)(n.li,{children:"Fallback to LLaMA service when needed"}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"// Example embedding usage\nconst embedding = await runtime.llamaService.getEmbeddingResponse(input);\n"})}),"\n",(0,s.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Model Selection"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Start with SMALL models and upgrade as needed"}),"\n",(0,s.jsx)(n.li,{children:"Use MEDIUM models as your default for general tasks"}),"\n",(0,s.jsx)(n.li,{children:"Reserve LARGE models for specific, complex requirements"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Parameter Tuning"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Keep temperature low (0.2-0.4) for consistent outputs"}),"\n",(0,s.jsx)(n.li,{children:"Increase temperature (0.6-0.8) for creative tasks"}),"\n",(0,s.jsx)(n.li,{children:"Adjust maxOutputTokens based on expected response length"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Embedding Optimization"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Utilize the caching system for frequently used content"}),"\n",(0,s.jsx)(n.li,{children:"Choose provider-specific embedding models for best results"}),"\n",(0,s.jsx)(n.li,{children:"Monitor embedding performance and adjust as needed"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Cost Optimization"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Use SMALL models for development and testing"}),"\n",(0,s.jsx)(n.li,{children:"Implement caching strategies for embeddings"}),"\n",(0,s.jsx)(n.li,{children:"Monitor token usage across different model classes"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"common-issues-and-solutions",children:"Common Issues and Solutions"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Token Length Errors"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"// Solution: Implement chunking for long inputs\nconst chunks = splitIntoChunks(input, model.settings.maxInputTokens);\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Response Quality Issues"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"// Solution: Adjust temperature and penalties\nconst enhancedSettings = {\n    ...defaultSettings,\n    temperature: 0.4,\n    presence_penalty: 0.2\n};\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Embedding Cache Misses"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"// Solution: Implement broader similarity thresholds\nconst similarityThreshold = 0.85;\nconst cachedEmbedding = await findSimilarEmbedding(input, similarityThreshold);\n"})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"advanced-configuration",children:"Advanced Configuration"}),"\n",(0,s.jsx)(n.p,{children:"For advanced use cases, you can extend the model configuration:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:'// Custom model configuration\nconst customConfig = {\n    model: {\n        [ModelClass.SMALL]: "your-custom-model",\n        [ModelClass.MEDIUM]: "your-custom-model",\n        [ModelClass.LARGE]: "your-custom-model",\n        [ModelClass.EMBEDDING]: "your-custom-embedding-model",\n    },\n    settings: {\n        // Custom settings\n        maxInputTokens: 64000,\n        temperature: 0.5,\n        // Add custom parameters\n        custom_param: "value"\n    }\n};\n'})}),"\n",(0,s.jsx)(n.h2,{id:"additional-resources",children:"Additional Resources"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Check the ",(0,s.jsx)(n.a,{href:"/docs/core/providers",children:"Model Providers"})," documentation for more details about specific providers"]}),"\n",(0,s.jsxs)(n.li,{children:["See ",(0,s.jsx)(n.a,{href:"/docs/guides/configuration",children:"Configuration Guide"})," for general configuration options"]}),"\n",(0,s.jsxs)(n.li,{children:["Visit ",(0,s.jsx)(n.a,{href:"/docs/guides/advanced",children:"Advanced Usage"})," for complex deployment scenarios"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Remember to monitor your model's performance and adjust these configurations based on your specific use case and requirements."})]})}function u(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(a,{...e})}):a(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>o});var s=i(6540);const l={},d=s.createContext(l);function r(e){const n=s.useContext(d);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(l):e.components||l:r(e.components),s.createElement(d.Provider,{value:n},e.children)}}}]);