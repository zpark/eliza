"use strict";(self.webpackChunk_elizaos_docs=self.webpackChunk_elizaos_docs||[]).push([[91924],{53255:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>a,contentTitle:()=>s,default:()=>p,frontMatter:()=>r,metadata:()=>i,toc:()=>d});var i=t(99991),o=t(31085),l=t(71184);const r={title:"Using OpenAI Plugin Envs for Any OpenAI-Compatible Provider",authors:"team",date:new Date("2025-04-24T00:00:00.000Z"),description:"Learn how to configure the OpenAI plugin to work with any OpenAI-compatible provider using environment variables."},s="\ud83d\ude80 Using OpenAI Plugin Envs Any OpenAI-Compatible Provider",a={authorsImageUrls:[void 0]},d=[{value:"\ud83e\udd14 What is an OpenAI-Compatible Provider?",id:"-what-is-an-openai-compatible-provider",level:2},{value:"\ud83d\udee0\ufe0f Key Environment Variables",id:"\ufe0f-key-environment-variables",level:2},{value:"Example: Connecting to OpenRouter",id:"example-connecting-to-openrouter",level:2},{value:"Example: Connecting to Ollama",id:"example-connecting-to-ollama",level:2},{value:"Example: Connecting to a Local LLM (Llama.cpp)",id:"example-connecting-to-a-local-llm-llamacpp",level:2},{value:"Example: Connecting to LM Studio",id:"example-connecting-to-lm-studio",level:2},{value:"Handling Providers Without Embedding Support",id:"handling-providers-without-embedding-support",level:2}];function c(e){const n={a:"a",blockquote:"blockquote",code:"code",em:"em",h2:"h2",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,l.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsxs)(n.p,{children:["The ",(0,o.jsx)(n.code,{children:"plugin-openai"})," package in this project can connect to any OpenAI-compatible API provider\u2014not just OpenAI itself! Thanks to flexible environment variable support, you can swap between providers like Azure, OpenRouter, or even your own local LLM with just a few tweaks. It\u2019s that easy!"]}),"\n","\n",(0,o.jsx)(n.h2,{id:"-what-is-an-openai-compatible-provider",children:"\ud83e\udd14 What is an OpenAI-Compatible Provider?"}),"\n",(0,o.jsx)(n.p,{children:"An OpenAI-compatible provider is any service that implements the OpenAI API spec. Popular examples include:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.a,{href:"https://openrouter.ai/",children:"OpenRouter"})}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.a,{href:"https://ollama.com/",children:"Ollama"})," (with its OpenAI-compatible API, supports embedding also)"]}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.a,{href:"https://github.com/abetlen/llama-cpp-python",children:"Local LLMs with an OpenAI API wrapper"})}),"\n",(0,o.jsx)(n.li,{children:"Other cloud or self-hosted endpoints"}),"\n"]}),"\n",(0,o.jsxs)(n.blockquote,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Note:"})," If your provider supports the OpenAI API, this plugin can probably talk to it!"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"\ufe0f-key-environment-variables",children:"\ud83d\udee0\ufe0f Key Environment Variables"}),"\n",(0,o.jsx)(n.p,{children:"The following environment variables are supported by the OpenAI plugin:"}),"\n",(0,o.jsxs)(n.table,{children:[(0,o.jsx)(n.thead,{children:(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.th,{children:"Variable"}),(0,o.jsx)(n.th,{children:"Purpose"})]})}),(0,o.jsxs)(n.tbody,{children:[(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.code,{children:"OPENAI_API_KEY"})}),(0,o.jsx)(n.td,{children:"The API key for authentication (required)"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.code,{children:"OPENAI_BASE_URL"})}),(0,o.jsx)(n.td,{children:"The base URL for the API (override to use other providers)"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.code,{children:"OPENAI_SMALL_MODEL"})}),(0,o.jsx)(n.td,{children:"Default small model name"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.code,{children:"OPENAI_LARGE_MODEL"})}),(0,o.jsx)(n.td,{children:"Default large model name"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.code,{children:"OPENAI_EMBEDDING_MODEL"})}),(0,o.jsx)(n.td,{children:"Embedding model name"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.code,{children:"OPENAI_EMBEDDING_URL"})}),(0,o.jsx)(n.td,{children:"Base URL specifically for the embedding API endpoint"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.code,{children:"OPENAI_EMBEDDING_DIMENSIONS"})}),(0,o.jsx)(n.td,{children:"Embedding vector dimensions"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.code,{children:"SMALL_MODEL"})}),(0,o.jsx)(n.td,{children:"(Fallback) Small model name"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.code,{children:"LARGE_MODEL"})}),(0,o.jsx)(n.td,{children:"(Fallback) Large model name"})]})]})]}),"\n",(0,o.jsx)(n.h2,{id:"example-connecting-to-openrouter",children:"Example: Connecting to OpenRouter"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-env",children:"OPENAI_API_KEY=your-openrouter-key\nOPENAI_BASE_URL=https://openrouter.ai/api/v1\nOPENAI_SMALL_MODEL=openrouter/gpt-3.5-turbo\nOPENAI_LARGE_MODEL=openrouter/gpt-4\n"})}),"\n",(0,o.jsxs)(n.blockquote,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Warning:"})," OpenRouter does ",(0,o.jsx)(n.strong,{children:"not"})," currently support the ",(0,o.jsx)(n.code,{children:"/v1/embeddings"})," endpoint. If you need embeddings, you must use a different provider for them. See the section below on ",(0,o.jsx)(n.a,{href:"#handling-providers-without-embedding-support",children:"Handling Providers Without Embedding Support"}),"."]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"example-connecting-to-ollama",children:"Example: Connecting to Ollama"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-env",children:"OPENAI_API_KEY=ollama-local-demo\nOPENAI_BASE_URL=http://localhost:11434/v1\nOPENAI_SMALL_MODEL=llama2\nOPENAI_LARGE_MODEL=llama2:70b\n"})}),"\n",(0,o.jsx)(n.h2,{id:"example-connecting-to-a-local-llm-llamacpp",children:"Example: Connecting to a Local LLM (Llama.cpp)"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-env",children:"OPENAI_API_KEY=sk-local-demo\nOPENAI_BASE_URL=http://localhost:8000/v1\nOPENAI_SMALL_MODEL=llama-2-7b-chat\nOPENAI_LARGE_MODEL=llama-2-13b-chat\n"})}),"\n",(0,o.jsx)(n.h2,{id:"example-connecting-to-lm-studio",children:"Example: Connecting to LM Studio"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.a,{href:"https://lmstudio.ai/",children:"LM Studio"})," is a popular desktop app for running large language models locally. It provides an OpenAI-compatible API server, so you can use it as a drop-in replacement for OpenAI or other providers."]}),"\n",(0,o.jsx)(n.p,{children:"To use LM Studio with the OpenAI plugin, start the LM Studio API server (default port 1234) and set your environment variables as follows:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-env",children:"OPENAI_API_KEY=lmstudio-local-demo # (can be any non-empty string)\nOPENAI_BASE_URL=http://localhost:1234/v1\nOPENAI_SMALL_MODEL=your-model-name-here\nOPENAI_LARGE_MODEL=your-model-name-here\nOPENAI_EMBEDDING_MODEL=text-embedding-nomic-embed-text-v1.5\n\n"})}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["Make sure to use the model identifier as listed in LM Studio for the ",(0,o.jsx)(n.code,{children:"OPENAI_SMALL_MODEL"})," and ",(0,o.jsx)(n.code,{children:"OPENAI_LARGE_MODEL"})," values."]}),"\n",(0,o.jsxs)(n.li,{children:["LM Studio supports the ",(0,o.jsx)(n.code,{children:"/v1/models"}),", ",(0,o.jsx)(n.code,{children:"/v1/chat/completions"}),", ",(0,o.jsx)(n.code,{children:"/v1/embeddings"}),", and ",(0,o.jsx)(n.code,{children:"/v1/completions"})," endpoints."]}),"\n",(0,o.jsxs)(n.li,{children:["You can reuse any OpenAI-compatible SDK by pointing the base URL to ",(0,o.jsx)(n.code,{children:"http://localhost:1234/v1"}),"."]}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:["For more details, see the ",(0,o.jsx)(n.a,{href:"https://lmstudio.ai/docs/app/api/endpoints/openai",children:"LM Studio OpenAI Compatibility API docs"}),"."]}),"\n",(0,o.jsx)(n.h2,{id:"handling-providers-without-embedding-support",children:"Handling Providers Without Embedding Support"}),"\n",(0,o.jsxs)(n.p,{children:["Some OpenAI-compatible providers (like OpenRouter) might not offer an embedding endpoint (",(0,o.jsx)(n.code,{children:"/v1/embeddings"}),"). If you need embedding functionality (e.g., for memory or context retrieval), you can configure the OpenAI plugin to use a ",(0,o.jsx)(n.em,{children:"different"})," provider specifically for embeddings using the ",(0,o.jsx)(n.code,{children:"OPENAI_EMBEDDING_URL"})," environment variable."]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Example: OpenRouter for Chat, Ollama for Embeddings"})}),"\n",(0,o.jsxs)(n.p,{children:["Let's say you want to use OpenRouter for its wide model selection for chat completion but prefer Ollama's embedding capabilities (see ",(0,o.jsx)(n.a,{href:"https://github.com/ollama/ollama/blob/main/docs/openai.md",children:"Ollama OpenAI docs"}),")."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-env",children:"# General API settings (points to OpenRouter)\nOPENAI_API_KEY=your-openrouter-key\nOPENAI_BASE_URL=https://openrouter.ai/api/v1\nOPENAI_SMALL_MODEL=openrouter/gpt-3.5-turbo\nOPENAI_LARGE_MODEL=openrouter/gpt-4\n\n# Embedding-specific settings (points to Ollama)\nOPENAI_EMBEDDING_URL=http://localhost:11434/v1 # Your Ollama embedding endpoint\nOPENAI_EMBEDDING_MODEL=all-minilm # Ollama embedding model (e.g., all-minilm)\n# OPENAI_EMBEDDING_DIMENSIONS=1536 # Optional: Specify if needed for your model\n"})})]})}function p(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(c,{...e})}):c(e)}},71184:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>s});var i=t(14041);const o={},l=i.createContext(o);function r(e){const n=i.useContext(l);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),i.createElement(l.Provider,{value:n},e.children)}},99991:e=>{e.exports=JSON.parse('{"permalink":"/blog/openai-plugin-envs","editUrl":"https://github.com/elizaos/eliza/tree/develop/docs/blog/openai-plugin-envs.mdx","source":"@site/blog/openai-plugin-envs.mdx","title":"Using OpenAI Plugin Envs for Any OpenAI-Compatible Provider","description":"Learn how to configure the OpenAI plugin to work with any OpenAI-compatible provider using environment variables.","date":"2025-04-24T00:00:00.000Z","tags":[],"readingTime":3.04,"hasTruncateMarker":true,"authors":[{"name":"ElizaOS Team","title":"Core Team","url":"https://github.com/elizaos","socials":{"twitter":"https://twitter.com/elizaOS","github":"https://github.com/elizaOS"},"imageURL":"https://github.com/elizaos.png","key":"team","page":null}],"frontMatter":{"title":"Using OpenAI Plugin Envs for Any OpenAI-Compatible Provider","authors":"team","date":"2025-04-24T00:00:00.000Z","description":"Learn how to configure the OpenAI plugin to work with any OpenAI-compatible provider using environment variables."},"unlisted":false,"lastUpdatedBy":"madjin","prevItem":{"title":"Autofun Tokenomics","permalink":"/blog/autofun-tokenomics"},"nextItem":{"title":"Adding Plugins in V2","permalink":"/blog/add-plugins"}}')}}]);